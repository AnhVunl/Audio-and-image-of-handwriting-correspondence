{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the data \n",
    "audio = np.load('spoken_train.npy')\n",
    "audio_test = np.load('spoken_test.npy') \n",
    "written = np.load('written_train.npy')\n",
    "match_train = np.load('match_train.npy') \n",
    "written_test = np.load('written_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features for the images \n",
    "written = written / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the optimal number of componets for dimensionality reduction for images \n",
    "for n_comp in [5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80]:\n",
    "    pca_wr = PCA(n_components = n_comp, random_state = 811)\n",
    "    pca_wr.fit(written)\n",
    "    pca_wr.transform(written)\n",
    "    print(\"For {} components variance is equal to {}\".format(n_comp, np.sum(pca_wr.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for images \n",
    "pca_wr = PCA(n_components = 50, random_state = 811)\n",
    "pca_wr.fit(written)\n",
    "pca_written = pca_wr.transform(written)  \n",
    "\n",
    "plt.plot(np.cumsum(pca_wr.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate mean,standard deviation, maximum, minimum per feature (out of the 13), per observation.\n",
    "def audio_features (spoken, functions):\n",
    "    return np.concatenate([np.array([function(i, axis = 0) for i in spoken]) for function in functions], axis = 1)\n",
    "\n",
    "summaries = [np.mean, np.max, np.min, np.std]\n",
    "audio_f = audio_features(audio, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge written and audio data\n",
    "both = np.hstack((pca_written, audio_f))\n",
    "both.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data \n",
    "scaler = StandardScaler()\n",
    "final = scaler.fit_transform(both)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data \n",
    "X_train, X_val, y_train, y_val = train_test_split(final, match_train, test_size = 0.1, random_state = 811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Y into categorical variable\n",
    "y_train = to_categorical(y_train)\n",
    "y_val   = to_categorical(y_val)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience= 100),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "def make_model(n_features):\n",
    "    model = Sequential()\n",
    "    optimizer = Adam(lr = 0.001)\n",
    "    \n",
    "    model.add(Dense(500, input_shape=(final.shape[1],),\n",
    "              kernel_initializer= 'glorot_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(400, kernel_initializer= 'glorot_uniform'))\n",
    "    model.add(LeakyReLU(alpha=0.01)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(300, kernel_initializer= 'glorot_uniform'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(200, kernel_initializer= 'glorot_normal'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer= optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = make_model(final.shape[1])\n",
    "history = model.fit(X_train, y_train, batch_size= 60 , epochs = 200, verbose=1, validation_data = (X_val, y_val), callbacks = callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print validation accuracy and loss accuracy\n",
    "score = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Validation loss:', score[0]) \n",
    "print('Validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the validation accuracy and validation loss against validation accuracy and training accuracy\n",
    "print(history.history.keys())\n",
    "\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the class distribution\n",
    "match_tr = pd.DataFrame(match_train)\n",
    "def labelling (frame): # this function is to change the True/False label to 1/0\n",
    "    if frame == False:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_tr['label'] = match_tr[0].apply(labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_tr['label'].value_counts() \n",
    "# true ~= 10% false, clearly class imbalance so we should keep an eye on this when evaluting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "y_pred = y_pred.argmax(axis = -1)\n",
    "y_vall  = y_val.argmax(axis = -1)\n",
    "\n",
    "confusion_matrix(y_vall, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the recall and precision , because we do not know which class is considered the thing wish to find we will\n",
    "#calculate precision and recall for both classes (0 and 1)\n",
    "r1 = recall_score(y_vall, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "r0 = recall_score(y_vall, y_pred, labels=None, pos_label=0, average='binary', sample_weight=None)\n",
    "p1 = precision_score(y_vall, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "p0  = precision_score(y_vall, y_pred, labels=None, pos_label=0, average='binary', sample_weight=None)\n",
    "print('Recall for class 1 is {}'.format(r1))\n",
    "print('Recall for class 0 is {}'.format(r0))\n",
    "print('Precision for class 1 is {}'.format(p1))\n",
    "print('Precision for class 0 is {}'.format(p0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make function for prediction on test_set \n",
    "def predict_test(sp_test, wr_test):\n",
    "    written_test = wr_test / 255\n",
    "    writ = pca_wr.transform(written_test)\n",
    "    audio_f= audio_features(sp_test, summaries)\n",
    "    both = np.hstack((writ, audio_f))\n",
    "    final = scaler.transform(both)\n",
    "    pred =  model.predict(final)\n",
    "    return pred.argmax(axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the above function\n",
    "pred_test = predict_test(audio_test, written_test)\n",
    "pred_test= pred_test.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it \n",
    "np.save(\"result.npy\", pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check it \n",
    "z = np.load(\"result.npy\")\n",
    "print(z[10:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use this code to find whether removing any of the audio features can increase validation accuracy,but the accurary did not improve\n",
    "#def best(text, audio):\n",
    "#    final = scaler.transform(np.hstack((text, audio)))\n",
    "#   X_train, X_val, y_train, y_val = train_test_split(final, match_train, test_size = 0.1, random_state = 811)\n",
    "#   y_train = to_categorical(y_train)\n",
    "#    y_val = to_categorical(y_val)\n",
    "#    model = make_model(final.shape[1])\n",
    "#   fit = model.fit(X_train, y_train, batch_size=60, epochs=200, verbose=1, validation_data = (X_val, y_val), callbacks = callbacks)\n",
    "#   score = model.evaluate(X_val, y_val, verbose=0)\n",
    "#   return score[1]\n",
    "\n",
    "#text = pca_written\n",
    "#audio1 = audio_features(audio, summaries)\n",
    "#acc = best(text, audio1)\n",
    "#print(\"All {}\".format(acc))\n",
    "\n",
    "#for i in range(len(summaries)):\n",
    "#    fs = summaries[:i] + summaries[i+1:]\n",
    "#    audio2 = audio_features(audio, fs)\n",
    "#    this = best(text, audio2)\n",
    "#    print(\"{} {}\".format(summaries[i].__name__, this))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
